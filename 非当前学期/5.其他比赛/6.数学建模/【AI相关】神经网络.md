神经网络的逻辑就是通过我们的原有的一些输入得到我们的最红的输入。

## 原理：
我们对于每一层，都涉及一个神经元，我们的这一个神经元由我们的**上一层的所有输入共同巨顶**，也就是说，我们有 $f(x_{i})=k_{1}x_{1}+\dots+k_{n}x_{n}$。对于我们的一个神经元来说，其都有数个输入和一个输出。

也就是说，对于单个神经元来说，我们的输出值 $y=f\left( \sum w_{i}x_{i}-\theta \right)$,这里我们的 $f$ 一般来说都是一个**0-1 函数**。这个时候

这个时候，我们的后续的目的，就是找到我们**每一个神经元对应的 $w$,以及我们的阈值**$\theta$。

### BP 算法：
我们通常说我们的 BP 网络的时候，一般是指用 BP 算法训练的多层前反馈神经网络。我们的 BP 算法，也叫做我们的误差逆传播。

假设我们有训练集 $[(x_{i},y_{i}),\dots(x_{m},y_{m})]$，并且我们的每一层都采用我们的 `Sigmoid` 函数，那么，我们假定我们的神经网络的输出 $y^{k}=(y_{1},y_{2},\dots y_{l})$。那么类似于前述的单个神经元，我们就有：

在我们的案例中，我们希望我们的均方误差 $E=\frac{1}{2}\sum(y-y_{i})^2$ 最小，这个时候，我们采用**梯度下降策略，对我们的参数进行调整**，我们的参数优化的目标就是要最小化我们的 $E$,在完成我们的 $k$ 轮迭代，假设我们已经得到了我们的 $w_{k}$,利用泰勒公式，我们可以把我们的 $w_{k}$ 展开，那么我们就有 $E(w)=E(w_{k})+E'(w_{k})(w-w_{k})$，之后进行的迭代之后，我们就徐网友：$E(w_{k+1})=E(w_{k})+E'(w_{k})(w_{k+1}-w_{k})$。我们希望我们的**新的误差更小**，那么我们就有 $E(w+1)-E(w)=E'(w_{k})(w_{k+1}-w_{k})<0$。

我们这个时候分类讨论：
如果我们的 $E'(w)>0$ 那么我们就需要 $w_{k+1}<w_{k}$，如果相反，那么我们就像余姚满足我们的 $w_{k+1}>w_{k}$。这个时候，我们引入一个很小的参数 $\eta_{0}$ 来表示我们**学习率**。

这个时候，我们就 i 有我们的迭代公式 $\delta w_{hj}=-\eta E'(w_{hj})=-\dfrac{\eta \partial E_{k}}{\partial w_{hj}}$。我们由我们的链式法则，我们直到，我们的 $\partial \frac{E_{k}}{w_{oj}}=\frac{\partial E_{k}\partial y}{\partial y\partial \beta_{j}}*\partial \frac{\beta_{j}}{\partial w_{hj}}$。

这个时候，我们于是就有我们的结果 $\delta w_{hj}=\eta g_{j}b_{h}$,其中我们的 $g_{j}=-(y_{calc}-y_{\mathrm{Re}al})*(y_{calc})'$。也就是说，我们的**预测误差越大，我们最后的调整的就越多**，其中我们的 $\beta_{h}$ 是我们的输入值。

类似的，我们的 $d\theta_{j}=-\eta g_{j}$。